{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tech Notes Tech Notes Run mkdocs to generate sites htmls Run mkdocs to generate sites htmls mkdocs build mkdocs serve # publish to git pages mkdocs gh-deploy It will be published on https://tanalex.github.io/tech_notes/","title":"Home"},{"location":"#tech-notes","text":"Tech Notes Run mkdocs to generate sites htmls","title":"Tech Notes"},{"location":"#run-mkdocs-to-generate-sites-htmls","text":"mkdocs build mkdocs serve # publish to git pages mkdocs gh-deploy It will be published on https://tanalex.github.io/tech_notes/","title":"Run mkdocs to generate sites htmls"},{"location":"AWS/CloudFormation/","text":"CloudFormation note https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html A tutorial guide: https://medium.com/boltops/a-simple-introduction-to-aws-cloudformation-part-1-1694a41ae59d Basic structure Resources Type Properties Functions !Ref Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: SecurityGroups: - !Ref InstanceSecurityGroup KeyName: mykey ImageId: '' InstanceSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 0.0.0.0/0 Parameters Parameters: KeyName: Description: The EC2 Key Pair to allow SSH access to the instance Type: 'AWS::EC2::KeyPair::KeyName' Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: SecurityGroups: - !Ref InstanceSecurityGroup - MyExistingSecurityGroup KeyName: !Ref KeyName ImageId: ami-7a11e213 InstanceSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 0.0.0.0/0 Get parameters like Ansible var_prompt Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access into the WordPress web server Type: AWS::EC2::KeyPair::KeyName WordPressUser: Default: admin NoEcho: true Description: The WordPress database admin account user name Type: String MinLength: 1 MaxLength: 16 AllowedPattern: \"[a-zA-Z][a-zA-Z0-9]*\" WebServerPort: Default: 8888 Description: TCP/IP port for the WordPress web server Type: Number MinValue: 1 MaxValue: 65535 Constraints using AllowedValues \"Parameters\" : { \"InstanceType\" : { \"Description\" : \"WebServer EC2 instance type\", \"Type\" : \"String\", \"Default\" : \"t2.small\", \"AllowedValues\" : [ \"t1.micro\", \"t2.nano\", ... ... ], \"ConstraintDescription\" : \"must be a valid EC2 instance type.\" } }, The Fn::GetAtt function takes two parameters, the logical name of the resource and the name of the attribute to be retrieved. For a full list of available attributes for resources, see Fn::GetAtt. You'll notice that the Fn::GetAtt function lists its two parameters in an array. For functions that take multiple parameters, you use an array to specify their parameters. Resources: myBucket: Type: 'AWS::S3::Bucket' myDistribution: Type: 'AWS::CloudFront::Distribution' Properties: DistributionConfig: Origins: - DomainName: !GetAtt - myBucket - DomainName Id: myS3Origin S3OriginConfig: {} Enabled: 'true' DefaultCacheBehavior: TargetOriginId: myS3Origin ForwardedValues: QueryString: 'false' ViewerProtocolPolicy: allow-all use the Fn::FindInMap function passing the name of the map, the value used to find the mapped value, and the label of the mapped value you want to return. Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instance Type: String Mappings: RegionMap: us-east-1: AMI: ami-76f0061f us-west-1: AMI: ami-655a0a20 eu-west-1: AMI: ami-7fd4e10b ap-southeast-1: AMI: ami-72621c20 ap-northeast-1: AMI: ami-8e08a38f Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: KeyName: !Ref KeyName ImageId: !FindInMap - RegionMap - !Ref 'AWS::Region' - AMI UserData: !Base64 '80' !Join function and outputs Outputs: InstallURL: Value: !Join - '' - - 'http://' - !GetAtt - ElasticLoadBalancer - DNSName - /wp-admin/install.php Description: Installation URL of the WordPress website WebsiteURL: Value: !Join - '' - - 'http://' - !GetAtt - ElasticLoadBalancer - DNSName","title":"CloudFormation"},{"location":"AWS/CloudFormation/#cloudformation-note","text":"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/gettingstarted.templatebasics.html A tutorial guide: https://medium.com/boltops/a-simple-introduction-to-aws-cloudformation-part-1-1694a41ae59d","title":"CloudFormation note"},{"location":"AWS/CloudFormation/#basic-structure","text":"Resources Type Properties Functions !Ref Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: SecurityGroups: - !Ref InstanceSecurityGroup KeyName: mykey ImageId: '' InstanceSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 0.0.0.0/0 Parameters Parameters: KeyName: Description: The EC2 Key Pair to allow SSH access to the instance Type: 'AWS::EC2::KeyPair::KeyName' Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: SecurityGroups: - !Ref InstanceSecurityGroup - MyExistingSecurityGroup KeyName: !Ref KeyName ImageId: ami-7a11e213 InstanceSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: GroupDescription: Enable SSH access via port 22 SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 0.0.0.0/0 Get parameters like Ansible var_prompt Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access into the WordPress web server Type: AWS::EC2::KeyPair::KeyName WordPressUser: Default: admin NoEcho: true Description: The WordPress database admin account user name Type: String MinLength: 1 MaxLength: 16 AllowedPattern: \"[a-zA-Z][a-zA-Z0-9]*\" WebServerPort: Default: 8888 Description: TCP/IP port for the WordPress web server Type: Number MinValue: 1 MaxValue: 65535 Constraints using AllowedValues \"Parameters\" : { \"InstanceType\" : { \"Description\" : \"WebServer EC2 instance type\", \"Type\" : \"String\", \"Default\" : \"t2.small\", \"AllowedValues\" : [ \"t1.micro\", \"t2.nano\", ... ... ], \"ConstraintDescription\" : \"must be a valid EC2 instance type.\" } },","title":"Basic structure"},{"location":"AWS/CloudFormation/#the-fngetatt-function","text":"takes two parameters, the logical name of the resource and the name of the attribute to be retrieved. For a full list of available attributes for resources, see Fn::GetAtt. You'll notice that the Fn::GetAtt function lists its two parameters in an array. For functions that take multiple parameters, you use an array to specify their parameters. Resources: myBucket: Type: 'AWS::S3::Bucket' myDistribution: Type: 'AWS::CloudFront::Distribution' Properties: DistributionConfig: Origins: - DomainName: !GetAtt - myBucket - DomainName Id: myS3Origin S3OriginConfig: {} Enabled: 'true' DefaultCacheBehavior: TargetOriginId: myS3Origin ForwardedValues: QueryString: 'false' ViewerProtocolPolicy: allow-all","title":"The Fn::GetAtt function"},{"location":"AWS/CloudFormation/#use-the-fnfindinmap-function","text":"passing the name of the map, the value used to find the mapped value, and the label of the mapped value you want to return. Parameters: KeyName: Description: Name of an existing EC2 KeyPair to enable SSH access to the instance Type: String Mappings: RegionMap: us-east-1: AMI: ami-76f0061f us-west-1: AMI: ami-655a0a20 eu-west-1: AMI: ami-7fd4e10b ap-southeast-1: AMI: ami-72621c20 ap-northeast-1: AMI: ami-8e08a38f Resources: Ec2Instance: Type: 'AWS::EC2::Instance' Properties: KeyName: !Ref KeyName ImageId: !FindInMap - RegionMap - !Ref 'AWS::Region' - AMI UserData: !Base64 '80'","title":"use the Fn::FindInMap function"},{"location":"AWS/CloudFormation/#join-function-and-outputs","text":"Outputs: InstallURL: Value: !Join - '' - - 'http://' - !GetAtt - ElasticLoadBalancer - DNSName - /wp-admin/install.php Description: Installation URL of the WordPress website WebsiteURL: Value: !Join - '' - - 'http://' - !GetAtt - ElasticLoadBalancer - DNSName","title":"!Join function and outputs"},{"location":"AWS/SnapshotManager/","text":"Snapshot Manager Note Overview of the problem we want to solve In dailly AWS operations, we need to make snapshot and copy them to another region for backup purpose. This is not a new problem and there are many existing solutions. But there is an interesting sub-problem that seems not being taken care of before. That sub-problem is how can we keep the snapshots efficiently. For example, if those snapshots were made every 30 minutes, you will have 48 snapshots in a day and 480 snapshots in 10 days. In previous solutions, they just simply setup a policy to remove those old snapshots by a fixed number or date. The problem is if you set to keep 30 snapshots, it will only keep less than a days backup and if you set to keep 10 days, you end up wasting a lot of storage to keep these 480 snapshots. A much better solution is to have an \"Elastic\" rule to remove those snapshots based on their create time. For example, we can set the rule like this: * If this snapshot was made in recent 5 hours, then we keep it * If it's between 5h to 12h, we keep only 1 per hour * If it's from 12h to 24h, we only keep one every 4 hours * If it's after 2days, we only keep 1 per day * If it's over 14 days, we don't keep This solution is like the RRDTools db algorithm, we keep the data based on their freshness. Older data will be treated differently than newer data. This algorithm is what we want to implement here. Simplify the problem first Lets try to solve a simplified version first then we can use the knowledge to implement a much complete solution. First, we set the data we want to process is just a simple array(LIST) with integer from 1 to 50 Data = [i+1 for i in range(50)] Then we define our rule like this Rule = { \"5\": 3, \"10\": 6, \"25\": 10, \"40\": -1 } This rule means: - If data >= 5, keep one every 3 - If data >=10, keep one every 6 - If data >=25, keep one every 10 - If data >=40, don't keep Then we sort the Rule's keys ruleKeys = sorted([ int(v) for v in rule.keys() ]) now ruleKeys is like [5, 10, 25, 40] for any given data as d , we will check which range it fits in the ruleKeys Here we use bisect.bisect_left to do a quick bisect search def find_lt(a, x): 'Find rightmost value less than x' i = bisect_left(a, x) if i: return a[i-1] raise ValueError For example, if data is 6 k = find_lt(ruleKeys, 6) will give you the rightmost value in ruleKeys less than 6 which is 5 Now we know 6 is in the range of 5 to 10, and based on the Rule dict, we know in that range, we want to keep one every 3 \"5\": 3 We call this 3 interval interval = rule[str(k)] Then we can calculate which \"bucket\" our data 6 will fit in bucketNumb = (d - k) // interval For example, (6 - 5) // 3 will give 0 and (8 - 5) // 3 will give 1 That means 5, 6, 7 will have same bucket number 0 and 8, 9, 10 will be in bucket 1 and 11, 12, 13 will be in bucket 2 We can just use the bucket number to decide whether we need to keep this data or not Say we have 5 then there is no need to keep 6 and 7 if we have 12, there is no need to keep 13 because they are in same bucket. So the ProcessData looks like this def ProcessData(data, rule): ruleKeys = sorted([ int(v) for v in rule.keys() ]) lastBucket = -1 lastRuleKey = -1 for d in data: ifRemove = False try: k = find_le(ruleKeys, d) interval = rule[str(k)] if interval <= 0: ifRemove = True else: if k == d: lastBucket = 0 lastRuleKey = k else: bucketNumb = (d - k) // interval if k != lastRuleKey: lastRuleKey = k lastBucket = bucketNumb else: if bucketNumb == lastBucket: ifRemove = True else: lastBucket = bucketNumb except ValueError: # k is less than ruleKeys[0], simply pass so it won't be REMOVED pass if ifRemove: print(\"{0:3d} {1:10s}\".format(d, \"REMOVE\")) else: print(\"{0:3d} {1:10s}\".format(d, \"OK\")) Next step, use this algorithm in the snapshot management program First, we need to set the Rule based on time difference of current time and the creation time of the snapshots # current time in UTC utc_now = datetime.now(timezone.utc) # time difference in seconds between \"now\" and the snapshots create time int((utc_now - s['StartTime']).total_seconds()) Next, we need to figure out a way to specify the Rules more intuitively We'd like the Rule to be readable, something like this: Rule = { \"10h\": \"20m\", \"11h10m\": 1800, \"12h\": \"40m\", \"1d\": \"1d\" \"2d\": -1 } In the example above, \"10h\" means 10 hours, 20m means 20 minutes, 1d means 1 day Here comes a handy utility function to convert these \"time string\" to regular timedelta regex = re.compile(r'((?P<days>\\d+?)d)?((?P<hours>\\d+?)h)?((?P<minutes>\\d+?)m)?((?P<seconds>\\d+?)s)?') def timestr_to_timedelta(time_str): # check if time_str is a numeric value like 1200 or -1 time_str = str(time_str) if re.match(r\"^\\d+$\", time_str): # if it's just an integer, assume it's seconds time_str += \"s\" # otherwise try to parse the value using RegEx parts = regex.search(time_str) if not parts: return None parts = parts.groupdict() time_params = {} for (name, param) in parts.items(): if param: time_params[name] = int(param) return timedelta(**time_params) Finally, we can combine all these to a more complete solution like: def process_snapshots(rule): # Get my AWS Account ID myAccount = boto3.client('sts').get_caller_identity()['Account'] # Connect to EC2 client = boto3.client('ec2', region_name = 'us-west-2') # Get a list of snapshots for my AWS account (not all public ones) snapshots = client.describe_snapshots(OwnerIds=[myAccount])['Snapshots'] print(snapshots) utc_now = datetime.now(timezone.utc) snapshots_seconds = [ { \"sec\": int((utc_now - s['StartTime']).total_seconds()), \"ind\": i } for i, s in enumerate(snapshots) ] # sort this list by the \"sec\" value, so it will be in asc order snapshots_seconds = sorted(snapshots_seconds, key=lambda x: x['sec']) print(snapshots_seconds) # process snapshots based on rules newRule = {} for k,v in rule.items(): rule_key = int(utils.timestr_to_timedelta(k).total_seconds()) rule_val = int(utils.timestr_to_timedelta(v).total_seconds()) newRule[str(rule_key)] = rule_val ruleKeys = sorted([ int(v) for v in newRule.keys() ]) rule = newRule lastBucket = -1 lastRuleKey = -1 for v in snapshots_seconds: d = v['sec'] i = v['ind'] ifRemove = False try: k = find_le(ruleKeys, d) interval = rule[str(k)] if interval <= 0: ifRemove = True else: if k == d: lastBucket = 0 lastRuleKey = k else: bucketNumb = (d - k) // interval if k != lastRuleKey: lastRuleKey = k lastBucket = bucketNumb else: if bucketNumb == lastBucket: ifRemove = True else: lastBucket = bucketNumb except ValueError: # k is less than ruleKeys[0], simply pass so it won't be REMOVED pass if ifRemove: print(\"{} sec:{} SnapshotId:{} {:10s}\".format(i, d, snapshots[i]['SnapshotId'], \"REMOVE\")) else: print(\"{} sec:{} SnapshotId:{} {:10s}\".format(i, d, snapshots[i]['SnapshotId'], \"OK\"))","title":"Snapshot manager"},{"location":"AWS/SnapshotManager/#snapshot-manager-note","text":"","title":"Snapshot Manager Note"},{"location":"AWS/SnapshotManager/#overview-of-the-problem-we-want-to-solve","text":"In dailly AWS operations, we need to make snapshot and copy them to another region for backup purpose. This is not a new problem and there are many existing solutions. But there is an interesting sub-problem that seems not being taken care of before. That sub-problem is how can we keep the snapshots efficiently. For example, if those snapshots were made every 30 minutes, you will have 48 snapshots in a day and 480 snapshots in 10 days. In previous solutions, they just simply setup a policy to remove those old snapshots by a fixed number or date. The problem is if you set to keep 30 snapshots, it will only keep less than a days backup and if you set to keep 10 days, you end up wasting a lot of storage to keep these 480 snapshots. A much better solution is to have an \"Elastic\" rule to remove those snapshots based on their create time. For example, we can set the rule like this: * If this snapshot was made in recent 5 hours, then we keep it * If it's between 5h to 12h, we keep only 1 per hour * If it's from 12h to 24h, we only keep one every 4 hours * If it's after 2days, we only keep 1 per day * If it's over 14 days, we don't keep This solution is like the RRDTools db algorithm, we keep the data based on their freshness. Older data will be treated differently than newer data. This algorithm is what we want to implement here.","title":"Overview of the problem we want to solve"},{"location":"AWS/SnapshotManager/#simplify-the-problem-first","text":"Lets try to solve a simplified version first then we can use the knowledge to implement a much complete solution. First, we set the data we want to process is just a simple array(LIST) with integer from 1 to 50 Data = [i+1 for i in range(50)] Then we define our rule like this Rule = { \"5\": 3, \"10\": 6, \"25\": 10, \"40\": -1 } This rule means: - If data >= 5, keep one every 3 - If data >=10, keep one every 6 - If data >=25, keep one every 10 - If data >=40, don't keep Then we sort the Rule's keys ruleKeys = sorted([ int(v) for v in rule.keys() ]) now ruleKeys is like [5, 10, 25, 40] for any given data as d , we will check which range it fits in the ruleKeys Here we use bisect.bisect_left to do a quick bisect search def find_lt(a, x): 'Find rightmost value less than x' i = bisect_left(a, x) if i: return a[i-1] raise ValueError For example, if data is 6 k = find_lt(ruleKeys, 6) will give you the rightmost value in ruleKeys less than 6 which is 5 Now we know 6 is in the range of 5 to 10, and based on the Rule dict, we know in that range, we want to keep one every 3 \"5\": 3 We call this 3 interval interval = rule[str(k)] Then we can calculate which \"bucket\" our data 6 will fit in bucketNumb = (d - k) // interval For example, (6 - 5) // 3 will give 0 and (8 - 5) // 3 will give 1 That means 5, 6, 7 will have same bucket number 0 and 8, 9, 10 will be in bucket 1 and 11, 12, 13 will be in bucket 2 We can just use the bucket number to decide whether we need to keep this data or not Say we have 5 then there is no need to keep 6 and 7 if we have 12, there is no need to keep 13 because they are in same bucket. So the ProcessData looks like this def ProcessData(data, rule): ruleKeys = sorted([ int(v) for v in rule.keys() ]) lastBucket = -1 lastRuleKey = -1 for d in data: ifRemove = False try: k = find_le(ruleKeys, d) interval = rule[str(k)] if interval <= 0: ifRemove = True else: if k == d: lastBucket = 0 lastRuleKey = k else: bucketNumb = (d - k) // interval if k != lastRuleKey: lastRuleKey = k lastBucket = bucketNumb else: if bucketNumb == lastBucket: ifRemove = True else: lastBucket = bucketNumb except ValueError: # k is less than ruleKeys[0], simply pass so it won't be REMOVED pass if ifRemove: print(\"{0:3d} {1:10s}\".format(d, \"REMOVE\")) else: print(\"{0:3d} {1:10s}\".format(d, \"OK\"))","title":"Simplify the problem first"},{"location":"AWS/SnapshotManager/#next-step-use-this-algorithm-in-the-snapshot-management-program","text":"First, we need to set the Rule based on time difference of current time and the creation time of the snapshots # current time in UTC utc_now = datetime.now(timezone.utc) # time difference in seconds between \"now\" and the snapshots create time int((utc_now - s['StartTime']).total_seconds()) Next, we need to figure out a way to specify the Rules more intuitively We'd like the Rule to be readable, something like this: Rule = { \"10h\": \"20m\", \"11h10m\": 1800, \"12h\": \"40m\", \"1d\": \"1d\" \"2d\": -1 } In the example above, \"10h\" means 10 hours, 20m means 20 minutes, 1d means 1 day Here comes a handy utility function to convert these \"time string\" to regular timedelta regex = re.compile(r'((?P<days>\\d+?)d)?((?P<hours>\\d+?)h)?((?P<minutes>\\d+?)m)?((?P<seconds>\\d+?)s)?') def timestr_to_timedelta(time_str): # check if time_str is a numeric value like 1200 or -1 time_str = str(time_str) if re.match(r\"^\\d+$\", time_str): # if it's just an integer, assume it's seconds time_str += \"s\" # otherwise try to parse the value using RegEx parts = regex.search(time_str) if not parts: return None parts = parts.groupdict() time_params = {} for (name, param) in parts.items(): if param: time_params[name] = int(param) return timedelta(**time_params) Finally, we can combine all these to a more complete solution like: def process_snapshots(rule): # Get my AWS Account ID myAccount = boto3.client('sts').get_caller_identity()['Account'] # Connect to EC2 client = boto3.client('ec2', region_name = 'us-west-2') # Get a list of snapshots for my AWS account (not all public ones) snapshots = client.describe_snapshots(OwnerIds=[myAccount])['Snapshots'] print(snapshots) utc_now = datetime.now(timezone.utc) snapshots_seconds = [ { \"sec\": int((utc_now - s['StartTime']).total_seconds()), \"ind\": i } for i, s in enumerate(snapshots) ] # sort this list by the \"sec\" value, so it will be in asc order snapshots_seconds = sorted(snapshots_seconds, key=lambda x: x['sec']) print(snapshots_seconds) # process snapshots based on rules newRule = {} for k,v in rule.items(): rule_key = int(utils.timestr_to_timedelta(k).total_seconds()) rule_val = int(utils.timestr_to_timedelta(v).total_seconds()) newRule[str(rule_key)] = rule_val ruleKeys = sorted([ int(v) for v in newRule.keys() ]) rule = newRule lastBucket = -1 lastRuleKey = -1 for v in snapshots_seconds: d = v['sec'] i = v['ind'] ifRemove = False try: k = find_le(ruleKeys, d) interval = rule[str(k)] if interval <= 0: ifRemove = True else: if k == d: lastBucket = 0 lastRuleKey = k else: bucketNumb = (d - k) // interval if k != lastRuleKey: lastRuleKey = k lastBucket = bucketNumb else: if bucketNumb == lastBucket: ifRemove = True else: lastBucket = bucketNumb except ValueError: # k is less than ruleKeys[0], simply pass so it won't be REMOVED pass if ifRemove: print(\"{} sec:{} SnapshotId:{} {:10s}\".format(i, d, snapshots[i]['SnapshotId'], \"REMOVE\")) else: print(\"{} sec:{} SnapshotId:{} {:10s}\".format(i, d, snapshots[i]['SnapshotId'], \"OK\"))","title":"Next step, use this algorithm in the snapshot management program"},{"location":"CSharp/","text":"C# Note Flagged enum [Flags] public enum Tag { None =0x0, Tip =0x1, Example=0x2 } snippet.Tag = Tag.Tip | Tag.Example Cast vs As CAST DESCRIPTION Tree tree = (Tree)obj Use this when you expect that obj will only ever be of type Tree. If obj is not a Tree, an InvalidCast exception will be raised. Tree tree = obj as Tree Use this when you anticipate that obj may or may not be a Tree. If obj is not a Tree, a null value will be assigned to tree. Always follow an \u201cas\u201d cast with conditional logic to properly handle the case where null is returned. Only use this style of conversion when necessary, since it necessitates conditional handling of the return value. This extra code creates opportunities for more bugs and makes the code more difficult to read and debug.","title":"C# Note"},{"location":"CSharp/#c-note","text":"","title":"C# Note"},{"location":"CSharp/#flagged-enum","text":"[Flags] public enum Tag { None =0x0, Tip =0x1, Example=0x2 } snippet.Tag = Tag.Tip | Tag.Example","title":"Flagged enum"},{"location":"CSharp/#cast-vs-as","text":"CAST DESCRIPTION Tree tree = (Tree)obj Use this when you expect that obj will only ever be of type Tree. If obj is not a Tree, an InvalidCast exception will be raised. Tree tree = obj as Tree Use this when you anticipate that obj may or may not be a Tree. If obj is not a Tree, a null value will be assigned to tree. Always follow an \u201cas\u201d cast with conditional logic to properly handle the case where null is returned. Only use this style of conversion when necessary, since it necessitates conditional handling of the return value. This extra code creates opportunities for more bugs and makes the code more difficult to read and debug.","title":"Cast vs As"},{"location":"DockerSwarm/","text":"Docker Swarm Stack with Traefik solution Docker Swarm Stack with Traefik solution Architect diagram High level explanation Traefik service act as load-balancer Frontier service example Backtier service example Steps to deploy and test Architect diagram High level explanation Lets say you have 8 VM Nodes showing as the Gray Box in the diagram above You plan to build a few fronttier web applications One web UI app written in TS or JS listening on port 5001 One Web REST API app writtern in DotNet Core listening on port 5002 One web GraphQL API app writtern in Javascript listening on port 5003 You want to use the following URL to access these services Web UI app: http://yourdomain.com/www REST API app: http://yourdomain.com/api GraphQL API app: http://yourdomain.com/graphql Traefik instance here can help, it works just like a reverse proxy. So based on a frontend rule like \"traefik.frontend.rule=PathPrefixStrip:/www , it will allow the request to be redirected/forwared to the backend service for the Web UI app This will be the same for backtier applications. You might want to run a few backtier app too One backend OSS app listening on port 60001 One backend Billing app listening on port 60002 One backend media app listing on port 60003 You want to access them from the fronttier containers using URL like these: Backend OSS app: http://backtier/oss/v1 Backend Billing app: http://backtier/billing/v1 Backend Media app: http://backtier/media/v1 Backend Media app version 2: http://backtier/media/v2 Now we need a 2nd Traefik instance to handle these ones Both of these 2 Traefik instances will listen on port 80 and 443, but only the fronttier Traefik instance docker container exposed/mapped to Node port 80 and 443, the backtier mapped to different ports or simply no need to map to Node Ports All the frontend containers can use http://backtier to access the backtier Traefik service anyway. Because we allow any container to deploy on any hosts to provide maximum flexibility, we will use a trick to differentiate which contains should be managed by which Traefik instance The way to achieve that is to use a tag on the Traefik service like \"--constraints=tag==traefik-fronttier\" All the containers with a tag like traefik-tags=traefik-fronttier will be associated only to the \"fronttier\" Traefik instance The same type of config works for backtier apps as well. Traefik service act as load-balancer There are 2 Traefik instances to handle both fronttier and backtier services Note: \"--constraints=tag==traefik-fronttier\" controls only the containers with traefik-tags=traefik-fronttier tags will be controlled by the fronttier traefik controller \"--constraints=tag==traefik-backtier\" controls only the containers with traefik-tags=traefik-backtier tags will be controlled by the backtier traefik controller restart_policy set to condition: any will allow these 2 traefik container/service to auto restart after node reboot or docker daemon reboot. constraints: [node.role == manager] will force these 2 traefik services to run only on Swarm Manager role nodes version: '3.3' networks: frontend: driver: overlay attachable: true volumes: data: services: fronttier: image: devmr1oktodock1:5000/traefik:1.7 command: - \"--docker\" - \"--docker.swarmmode=true\" - \"--docker.domain=docker.localhost\" - \"--docker.watch=true\" - \"--docker.exposedbydefault=true\" - \"--docker.endpoint=unix:///var/run/docker.sock\" - \"--constraints=tag==traefik-fronttier\" - \"--web\" ports: - \"80:80\" # The HTTP port - \"443:443\" - \"8000:8080\" # API volumes: - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events networks: - frontend labels: - \"traefik.enable=false\" deploy: placement: constraints: [node.role == manager] restart_policy: #condition: on-failure condition: any backtier: image: devmr1oktodock1:5000/traefik:1.7 command: - \"--docker\" - \"--docker.swarmmode=true\" - \"--docker.domain=docker.localhost\" - \"--docker.watch=true\" - \"--docker.exposedbydefault=true\" - \"--docker.endpoint=unix:///var/run/docker.sock\" - \"--constraints=tag==traefik-backtier\" - \"--web\" - \"--loglevel=DEBUG\" ports: - \"7180:80\" # The HTTP port - \"7443:443\" - \"7880:8080\" # API volumes: - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events networks: - frontend labels: - \"traefik.enable=false\" deploy: placement: constraints: [node.role == manager] restart_policy: condition: any Frontier service example The following simulate a simple fronttier webapp Note: extra_hosts are the list of the hosts Docker will put them into the container's /etc/hosts file This sample lists external IP for the redis , mongodb and consul services. extra_hosts will allow apps in docker container to communicate to external services constraints: - node.role == worker set the service contains to run only in nodes whose role is a worker - \"traefik.basic.port=5000\" this is the port for our webapp:1.1 service. This webapp listens and exposes on port 5000 - \"traefik.frontend.rule=PathPrefixStrip:/webapp\" this is Traefik setting for URL mapping. Any request to /webapp will be forwarded to port 5000 (the webapp service itself) \"traefik.backend=webapp\" set the name of the service Traefik will forward the request to, which is the service itself - \"traefik.docker.network=okto_frontend\" this is the Docker overlay network which Traefik and all services use to communicate - \"traefik.tags=traefik-fronttier\" is to set the webapp service to associate with the traefik-fronttier Traefik instance version: \"3.3\" services: webapp: image: devmr1oktodock1:5000/webapp:1.1 extra_hosts: - \"redis:172.25.83.76\" - \"mongodb:172.25.83.64\" - \"consul:172.25.83.61\" networks: - frontend deploy: placement: constraints: - node.role == worker restart_policy: condition: on-failure labels: - \"traefik.enable=true\" - \"traefik.basic.port=5000\" - \"traefik.basic.protocol=http\" - \"traefik.backend=webapp\" - \"traefik.frontend.rule=PathPrefixStrip:/webapp\" - \"traefik.docker.network=okto_frontend\" - \"traefik.backend.loadbalancer.swarm=true\" - \"traefik.tags=traefik-fronttier\" Backtier service example whoami: image: devmr1oktodock1:5000/whoami:latest extra_hosts: - \"redis:172.25.83.76\" - \"mongodb:172.25.83.64\" - \"consul:172.25.83.61\" networks: - frontend deploy: placement: constraints: - node.role == worker restart_policy: condition: on-failure labels: - \"traefik.enable=true\" - \"traefik.basic.port=80\" - \"traefik.basic.protocol=http\" - \"traefik.backend=whoami\" - \"traefik.frontend.rule=PathPrefixStrip:/whoami\" - \"traefik.docker.network=okto_frontend\" - \"traefik.backend.loadbalancer.swarm=true\" - \"traefik.tags=traefik-backtier\" networks: frontend: driver: overlay attachable: true Steps to deploy and test #Run this to deploy docker stack deploy -c docker-traefik-without-local-volumn.yml okto #Check if 2 instance's API interface (it was started using the --web flag in the yml) curl --noproxy '*' http://devmr1oktodock1:8000/api curl --noproxy '*' http://devmr1oktodock1:7880/api Once these 2 Traefik instances are running well, deploy both fronttier and backtier apps #Run this to deploy docker stack deploy -c docker-webapp.yml okto #Check the services curl --noproxy '*' http://devmr1oktodock1/webapp curl --noproxy '*' http://devmr1oktodock1:7180/whoami The following commands will scale the services to run multiple instances then check their status [root@devmr1oktodock1 multiple-traefik-stack]# docker service scale okto_webapp=3 okto_webapp scaled to 3 overall progress: 3 out of 3 tasks 1/3: running [==================================================>] 2/3: running [==================================================>] 3/3: running [==================================================>] verify: Service converged [root@devmr1oktodock1 multiple-traefik-stack]# docker service ps okto_webapp ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ldae6tup6v8s okto_webapp.1 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 37 minutes ago etexcwod5tux okto_webapp.2 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 14 seconds ago klopqgsbw2pe okto_webapp.3 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 14 seconds ago [root@devmr1oktodock1 multiple-traefik-stack]# docker service ps okto_whoami ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ojzt1rgpxx4i okto_whoami.1 devmr1oktodock1:5000/whoami:latest devmr1oktodock2.br.devrep.tv.telus.net Running Running 10 minutes ago ji9aga2xq4e0 okto_whoami.2 devmr1oktodock1:5000/whoami:latest devmr1oktodock1.br.devrep.tv.telus.net Running Running 9 minutes ago jbnkom5oiv2a okto_whoami.3 devmr1oktodock1:5000/whoami:latest devmr1oktodock3.br.devrep.tv.telus.net Running Running 9 minutes ago ww7315qshpzd okto_whoami.4 devmr1oktodock1:5000/whoami:latest devmr1oktodock3.br.devrep.tv.telus.net Running Running 9 minutes ago","title":"DockerSwarm"},{"location":"DockerSwarm/#docker-swarm-stack-with-traefik-solution","text":"Docker Swarm Stack with Traefik solution Architect diagram High level explanation Traefik service act as load-balancer Frontier service example Backtier service example Steps to deploy and test","title":"Docker Swarm Stack with Traefik solution"},{"location":"DockerSwarm/#architect-diagram","text":"","title":"Architect diagram"},{"location":"DockerSwarm/#high-level-explanation","text":"Lets say you have 8 VM Nodes showing as the Gray Box in the diagram above You plan to build a few fronttier web applications One web UI app written in TS or JS listening on port 5001 One Web REST API app writtern in DotNet Core listening on port 5002 One web GraphQL API app writtern in Javascript listening on port 5003 You want to use the following URL to access these services Web UI app: http://yourdomain.com/www REST API app: http://yourdomain.com/api GraphQL API app: http://yourdomain.com/graphql Traefik instance here can help, it works just like a reverse proxy. So based on a frontend rule like \"traefik.frontend.rule=PathPrefixStrip:/www , it will allow the request to be redirected/forwared to the backend service for the Web UI app This will be the same for backtier applications. You might want to run a few backtier app too One backend OSS app listening on port 60001 One backend Billing app listening on port 60002 One backend media app listing on port 60003 You want to access them from the fronttier containers using URL like these: Backend OSS app: http://backtier/oss/v1 Backend Billing app: http://backtier/billing/v1 Backend Media app: http://backtier/media/v1 Backend Media app version 2: http://backtier/media/v2 Now we need a 2nd Traefik instance to handle these ones Both of these 2 Traefik instances will listen on port 80 and 443, but only the fronttier Traefik instance docker container exposed/mapped to Node port 80 and 443, the backtier mapped to different ports or simply no need to map to Node Ports All the frontend containers can use http://backtier to access the backtier Traefik service anyway. Because we allow any container to deploy on any hosts to provide maximum flexibility, we will use a trick to differentiate which contains should be managed by which Traefik instance The way to achieve that is to use a tag on the Traefik service like \"--constraints=tag==traefik-fronttier\" All the containers with a tag like traefik-tags=traefik-fronttier will be associated only to the \"fronttier\" Traefik instance The same type of config works for backtier apps as well.","title":"High level explanation"},{"location":"DockerSwarm/#traefik-service-act-as-load-balancer","text":"There are 2 Traefik instances to handle both fronttier and backtier services Note: \"--constraints=tag==traefik-fronttier\" controls only the containers with traefik-tags=traefik-fronttier tags will be controlled by the fronttier traefik controller \"--constraints=tag==traefik-backtier\" controls only the containers with traefik-tags=traefik-backtier tags will be controlled by the backtier traefik controller restart_policy set to condition: any will allow these 2 traefik container/service to auto restart after node reboot or docker daemon reboot. constraints: [node.role == manager] will force these 2 traefik services to run only on Swarm Manager role nodes version: '3.3' networks: frontend: driver: overlay attachable: true volumes: data: services: fronttier: image: devmr1oktodock1:5000/traefik:1.7 command: - \"--docker\" - \"--docker.swarmmode=true\" - \"--docker.domain=docker.localhost\" - \"--docker.watch=true\" - \"--docker.exposedbydefault=true\" - \"--docker.endpoint=unix:///var/run/docker.sock\" - \"--constraints=tag==traefik-fronttier\" - \"--web\" ports: - \"80:80\" # The HTTP port - \"443:443\" - \"8000:8080\" # API volumes: - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events networks: - frontend labels: - \"traefik.enable=false\" deploy: placement: constraints: [node.role == manager] restart_policy: #condition: on-failure condition: any backtier: image: devmr1oktodock1:5000/traefik:1.7 command: - \"--docker\" - \"--docker.swarmmode=true\" - \"--docker.domain=docker.localhost\" - \"--docker.watch=true\" - \"--docker.exposedbydefault=true\" - \"--docker.endpoint=unix:///var/run/docker.sock\" - \"--constraints=tag==traefik-backtier\" - \"--web\" - \"--loglevel=DEBUG\" ports: - \"7180:80\" # The HTTP port - \"7443:443\" - \"7880:8080\" # API volumes: - /var/run/docker.sock:/var/run/docker.sock # So that Traefik can listen to the Docker events networks: - frontend labels: - \"traefik.enable=false\" deploy: placement: constraints: [node.role == manager] restart_policy: condition: any","title":"Traefik service act as load-balancer"},{"location":"DockerSwarm/#frontier-service-example","text":"The following simulate a simple fronttier webapp Note: extra_hosts are the list of the hosts Docker will put them into the container's /etc/hosts file This sample lists external IP for the redis , mongodb and consul services. extra_hosts will allow apps in docker container to communicate to external services constraints: - node.role == worker set the service contains to run only in nodes whose role is a worker - \"traefik.basic.port=5000\" this is the port for our webapp:1.1 service. This webapp listens and exposes on port 5000 - \"traefik.frontend.rule=PathPrefixStrip:/webapp\" this is Traefik setting for URL mapping. Any request to /webapp will be forwarded to port 5000 (the webapp service itself) \"traefik.backend=webapp\" set the name of the service Traefik will forward the request to, which is the service itself - \"traefik.docker.network=okto_frontend\" this is the Docker overlay network which Traefik and all services use to communicate - \"traefik.tags=traefik-fronttier\" is to set the webapp service to associate with the traefik-fronttier Traefik instance version: \"3.3\" services: webapp: image: devmr1oktodock1:5000/webapp:1.1 extra_hosts: - \"redis:172.25.83.76\" - \"mongodb:172.25.83.64\" - \"consul:172.25.83.61\" networks: - frontend deploy: placement: constraints: - node.role == worker restart_policy: condition: on-failure labels: - \"traefik.enable=true\" - \"traefik.basic.port=5000\" - \"traefik.basic.protocol=http\" - \"traefik.backend=webapp\" - \"traefik.frontend.rule=PathPrefixStrip:/webapp\" - \"traefik.docker.network=okto_frontend\" - \"traefik.backend.loadbalancer.swarm=true\" - \"traefik.tags=traefik-fronttier\"","title":"Frontier service example"},{"location":"DockerSwarm/#backtier-service-example","text":"whoami: image: devmr1oktodock1:5000/whoami:latest extra_hosts: - \"redis:172.25.83.76\" - \"mongodb:172.25.83.64\" - \"consul:172.25.83.61\" networks: - frontend deploy: placement: constraints: - node.role == worker restart_policy: condition: on-failure labels: - \"traefik.enable=true\" - \"traefik.basic.port=80\" - \"traefik.basic.protocol=http\" - \"traefik.backend=whoami\" - \"traefik.frontend.rule=PathPrefixStrip:/whoami\" - \"traefik.docker.network=okto_frontend\" - \"traefik.backend.loadbalancer.swarm=true\" - \"traefik.tags=traefik-backtier\" networks: frontend: driver: overlay attachable: true","title":"Backtier service example"},{"location":"DockerSwarm/#steps-to-deploy-and-test","text":"#Run this to deploy docker stack deploy -c docker-traefik-without-local-volumn.yml okto #Check if 2 instance's API interface (it was started using the --web flag in the yml) curl --noproxy '*' http://devmr1oktodock1:8000/api curl --noproxy '*' http://devmr1oktodock1:7880/api Once these 2 Traefik instances are running well, deploy both fronttier and backtier apps #Run this to deploy docker stack deploy -c docker-webapp.yml okto #Check the services curl --noproxy '*' http://devmr1oktodock1/webapp curl --noproxy '*' http://devmr1oktodock1:7180/whoami The following commands will scale the services to run multiple instances then check their status [root@devmr1oktodock1 multiple-traefik-stack]# docker service scale okto_webapp=3 okto_webapp scaled to 3 overall progress: 3 out of 3 tasks 1/3: running [==================================================>] 2/3: running [==================================================>] 3/3: running [==================================================>] verify: Service converged [root@devmr1oktodock1 multiple-traefik-stack]# docker service ps okto_webapp ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ldae6tup6v8s okto_webapp.1 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 37 minutes ago etexcwod5tux okto_webapp.2 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 14 seconds ago klopqgsbw2pe okto_webapp.3 devmr1oktodock1:5000/webapp:1.1 devmr1oktodock3.br.devrep.tv.telus.net Running Running 14 seconds ago [root@devmr1oktodock1 multiple-traefik-stack]# docker service ps okto_whoami ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ojzt1rgpxx4i okto_whoami.1 devmr1oktodock1:5000/whoami:latest devmr1oktodock2.br.devrep.tv.telus.net Running Running 10 minutes ago ji9aga2xq4e0 okto_whoami.2 devmr1oktodock1:5000/whoami:latest devmr1oktodock1.br.devrep.tv.telus.net Running Running 9 minutes ago jbnkom5oiv2a okto_whoami.3 devmr1oktodock1:5000/whoami:latest devmr1oktodock3.br.devrep.tv.telus.net Running Running 9 minutes ago ww7315qshpzd okto_whoami.4 devmr1oktodock1:5000/whoami:latest devmr1oktodock3.br.devrep.tv.telus.net Running Running 9 minutes ago","title":"Steps to deploy and test"},{"location":"Kubernetes/KubernetesNotes/","text":"Notes","title":"Kubernetes"},{"location":"Kubernetes/KubernetesNotes/#notes","text":"","title":"Notes"},{"location":"Powershell/","text":"Powershell Note Powershell Note Awsome Powershell and other useful sites [Powershell Modules Note](Module.md) Awsome Powershell and other useful sites Awsome Powershell Powershell Modules Note Powershell Functions","title":"Powershell Note"},{"location":"Powershell/#powershell-note","text":"Powershell Note Awsome Powershell and other useful sites [Powershell Modules Note](Module.md)","title":"Powershell Note"},{"location":"Powershell/#awsome-powershell-and-other-useful-sites","text":"Awsome Powershell","title":"Awsome Powershell and other useful sites"},{"location":"Powershell/#powershell-modules-note","text":"","title":"Powershell Modules Note"},{"location":"Powershell/#powershell-functions","text":"","title":"Powershell Functions"},{"location":"Powershell/Functions/","text":"Powershell functions notes Powershell functions notes ParameterSet and CmdletBinding Use Invoke-RestMethod to get GitHubOAuth Token ParameterSet and CmdletBinding Notable points: DefaultParameterSetName [Parameter(Mandatory = $false, Position=0, ParameterSetName='repo')] [ValidatePattern('^*$|^none$|^.+$')] [ValidateSet('open', 'closed')] function Get-GitHubIssues { [CmdletBinding(DefaultParameterSetName='repo')] param( [Parameter(Mandatory = $false, Position=0, ParameterSetName='repo')] [string] $Owner = $null, [Parameter(Mandatory = $false, Position=1, ParameterSetName='repo')] [string] $Repository = $null, [Parameter(Mandatory = $false, ParameterSetName='user')] [switch] $ForUser, [Parameter(Mandatory = $false)] [ValidateSet('open', 'closed')] $State = 'open', [Parameter(Mandatory = $false, ParameterSetName='user')] [ValidateSet('assigned', 'created', 'mentioned', 'subscribed')] $Filter = 'assigned', [Parameter(Mandatory = $false, ParameterSetName='repo')] [ValidatePattern('^\\*$|^none$|^\\d+$')] $Milestone, [Parameter(Mandatory = $false, ParameterSetName='repo')] [ValidatePattern('^\\*$|^none$|^.+$')] $Assignee, [Parameter(Mandatory = $false, ParameterSetName='repo')] [string] $Creator, [Parameter(Mandatory = $false, ParameterSetName='repo')] [string] $Mentioned, [Parameter(Mandatory = $false)] [string[]] $Labels = @(), [Parameter(Mandatory = $false)] [ValidateSet('created', 'updated', 'comments')] $Sort = 'created', [Parameter(Mandatory = $false)] [ValidateSet('asc', 'desc')] $Direction = 'desc', [Parameter(Mandatory = $false)] [DateTime] #Optional string of a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ $Since ) Use Invoke-RestMethod to get GitHubOAuth Token it's using basic auth, use GetBytes to convert string to bytes then use ToBase64String to convert them to base64 string function Get-GitHubOAuthTokens { [CmdletBinding()] param( [Parameter(Mandatory = $true)] [string] $UserName, [Parameter(Mandatory = $true)] [string] $Password ) try { $params = @{ Uri = 'https://api.github.com/authorizations'; Headers = @{ Authorization = 'Basic ' + [Convert]::ToBase64String( [Text.Encoding]::ASCII.GetBytes(\"$($userName):$($password)\")); } } $global:GITHUB_API_OUTPUT = Invoke-RestMethod @params #Write-Verbose $global:GITHUB_API_OUTPUT $global:GITHUB_API_OUTPUT | % { $date = [DateTime]::Parse($_.created_at).ToString('g') Write-Host \"`n$($_.app.name) - Created $date\" Write-Host \"`t$($_.token)`n`t$($_.app.url)\" } } catch { Write-Error \"An unexpected error occurred (bad user/password?) $($Error[0])\" } }","title":"PS Functions Note"},{"location":"Powershell/Functions/#powershell-functions-notes","text":"Powershell functions notes ParameterSet and CmdletBinding Use Invoke-RestMethod to get GitHubOAuth Token","title":"Powershell functions notes"},{"location":"Powershell/Functions/#parameterset-and-cmdletbinding","text":"Notable points: DefaultParameterSetName [Parameter(Mandatory = $false, Position=0, ParameterSetName='repo')] [ValidatePattern('^*$|^none$|^.+$')] [ValidateSet('open', 'closed')] function Get-GitHubIssues { [CmdletBinding(DefaultParameterSetName='repo')] param( [Parameter(Mandatory = $false, Position=0, ParameterSetName='repo')] [string] $Owner = $null, [Parameter(Mandatory = $false, Position=1, ParameterSetName='repo')] [string] $Repository = $null, [Parameter(Mandatory = $false, ParameterSetName='user')] [switch] $ForUser, [Parameter(Mandatory = $false)] [ValidateSet('open', 'closed')] $State = 'open', [Parameter(Mandatory = $false, ParameterSetName='user')] [ValidateSet('assigned', 'created', 'mentioned', 'subscribed')] $Filter = 'assigned', [Parameter(Mandatory = $false, ParameterSetName='repo')] [ValidatePattern('^\\*$|^none$|^\\d+$')] $Milestone, [Parameter(Mandatory = $false, ParameterSetName='repo')] [ValidatePattern('^\\*$|^none$|^.+$')] $Assignee, [Parameter(Mandatory = $false, ParameterSetName='repo')] [string] $Creator, [Parameter(Mandatory = $false, ParameterSetName='repo')] [string] $Mentioned, [Parameter(Mandatory = $false)] [string[]] $Labels = @(), [Parameter(Mandatory = $false)] [ValidateSet('created', 'updated', 'comments')] $Sort = 'created', [Parameter(Mandatory = $false)] [ValidateSet('asc', 'desc')] $Direction = 'desc', [Parameter(Mandatory = $false)] [DateTime] #Optional string of a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ $Since )","title":"ParameterSet and CmdletBinding"},{"location":"Powershell/Functions/#use-invoke-restmethod-to-get-githuboauth-token","text":"it's using basic auth, use GetBytes to convert string to bytes then use ToBase64String to convert them to base64 string function Get-GitHubOAuthTokens { [CmdletBinding()] param( [Parameter(Mandatory = $true)] [string] $UserName, [Parameter(Mandatory = $true)] [string] $Password ) try { $params = @{ Uri = 'https://api.github.com/authorizations'; Headers = @{ Authorization = 'Basic ' + [Convert]::ToBase64String( [Text.Encoding]::ASCII.GetBytes(\"$($userName):$($password)\")); } } $global:GITHUB_API_OUTPUT = Invoke-RestMethod @params #Write-Verbose $global:GITHUB_API_OUTPUT $global:GITHUB_API_OUTPUT | % { $date = [DateTime]::Parse($_.created_at).ToString('g') Write-Host \"`n$($_.app.name) - Created $date\" Write-Host \"`t$($_.token)`n`t$($_.app.url)\" } } catch { Write-Error \"An unexpected error occurred (bad user/password?) $($Error[0])\" } }","title":"Use Invoke-RestMethod to get GitHubOAuth Token"},{"location":"Powershell/Module/","text":"PS Modules Use Set-Alias to link function in module to a script Typically, functions are in the psm1 file but you can write in regular script files then use Set-Alias to link them in the psm1 module file Set-Alias -Name Build-Checkpoint -Value (Join-Path $PSScriptRoot Build-Checkpoint.ps1) Set-Alias -Name Build-Parallel -Value (Join-Path $PSScriptRoot Build-Parallel.ps1) Set-Alias -Name Invoke-Build -Value (Join-Path $PSScriptRoot Invoke-Build.ps1) Export-ModuleMember -Alias Build-Checkpoint, Build-Parallel, Invoke-Build Import all function scripts in a folder This snipet is from PSSlack.psm1 #Get public and private function definition files. $Public = @( Get-ChildItem -Path $PSScriptRoot\\Public\\*.ps1 -ErrorAction SilentlyContinue ) $Private = @( Get-ChildItem -Path $PSScriptRoot\\Private\\*.ps1 -ErrorAction SilentlyContinue ) $ModuleRoot = $PSScriptRoot #Dot source the files Foreach($import in @($Public + $Private)) { Try { . $import.fullname } Catch { Write-Error -Message \"Failed to import function $($import.fullname): $_\" } } Beginning in PowerShell 3.0, there is a new automatic variable available called $PSScriptRoot. This variable previously was only available within modules. It always points to the folder the current script is located in (so it only starts to be useful once you actually save a script before you run it). You can use $PSScriptRoot to load additional resources relative to your script location. For example, if you decide to place some functions in a separate \"library\" script that is located in the same folder, this would load the library script and import all of its functions The following snipet handles PS v2 situation #handle PS2 if(-not $PSScriptRoot) { $PSScriptRoot = Split-Path $MyInvocation.MyCommand.Path -Parent } Export functions # $Public is an array imported above Export-ModuleMember -Function $Public.Basename -Variable _PSSlackColorMap Export using wild-cards Export-ModuleMember -Function 'Get-*'","title":"PS Modules Note"},{"location":"Powershell/Module/#ps-modules","text":"","title":"PS Modules"},{"location":"Powershell/Module/#use-set-alias-to-link-function-in-module-to-a-script","text":"Typically, functions are in the psm1 file but you can write in regular script files then use Set-Alias to link them in the psm1 module file Set-Alias -Name Build-Checkpoint -Value (Join-Path $PSScriptRoot Build-Checkpoint.ps1) Set-Alias -Name Build-Parallel -Value (Join-Path $PSScriptRoot Build-Parallel.ps1) Set-Alias -Name Invoke-Build -Value (Join-Path $PSScriptRoot Invoke-Build.ps1) Export-ModuleMember -Alias Build-Checkpoint, Build-Parallel, Invoke-Build","title":"Use Set-Alias to link function in module to a script"},{"location":"Powershell/Module/#import-all-function-scripts-in-a-folder","text":"This snipet is from PSSlack.psm1 #Get public and private function definition files. $Public = @( Get-ChildItem -Path $PSScriptRoot\\Public\\*.ps1 -ErrorAction SilentlyContinue ) $Private = @( Get-ChildItem -Path $PSScriptRoot\\Private\\*.ps1 -ErrorAction SilentlyContinue ) $ModuleRoot = $PSScriptRoot #Dot source the files Foreach($import in @($Public + $Private)) { Try { . $import.fullname } Catch { Write-Error -Message \"Failed to import function $($import.fullname): $_\" } } Beginning in PowerShell 3.0, there is a new automatic variable available called $PSScriptRoot. This variable previously was only available within modules. It always points to the folder the current script is located in (so it only starts to be useful once you actually save a script before you run it). You can use $PSScriptRoot to load additional resources relative to your script location. For example, if you decide to place some functions in a separate \"library\" script that is located in the same folder, this would load the library script and import all of its functions The following snipet handles PS v2 situation #handle PS2 if(-not $PSScriptRoot) { $PSScriptRoot = Split-Path $MyInvocation.MyCommand.Path -Parent }","title":"Import all function scripts in a folder"},{"location":"Powershell/Module/#export-functions","text":"# $Public is an array imported above Export-ModuleMember -Function $Public.Basename -Variable _PSSlackColorMap Export using wild-cards Export-ModuleMember -Function 'Get-*'","title":"Export functions"},{"location":"Sites/","text":"Interesting Sites Interesting Sites CMSWing PDMan \u6570\u636e\u5e93\u5efa\u6a21 Dotjs a simple template CMSWing \u4e00\u6b3e\u57fa\u4e8eThinkJS(Node.js MVC)\u548cMySQL\u7684\u529f\u80fd\u5f3a\u5927\u7684\uff08PC\u7aef,\u624b\u673a\u7aef\u548c\u5fae\u4fe1\u516c\u4f17\u5e73\u53f0\uff09\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u53caCMS\u5efa\u7ad9\u7cfb\u7edf https://github.com/arterli/CmsWing PDMan \u6570\u636e\u5e93\u5efa\u6a21 https://gitee.com/robergroup/pdman Dotjs a simple template Dotjs","title":"Sites"},{"location":"Sites/#interesting-sites","text":"Interesting Sites CMSWing PDMan \u6570\u636e\u5e93\u5efa\u6a21 Dotjs a simple template","title":"Interesting Sites"},{"location":"Sites/#cmswing","text":"\u4e00\u6b3e\u57fa\u4e8eThinkJS(Node.js MVC)\u548cMySQL\u7684\u529f\u80fd\u5f3a\u5927\u7684\uff08PC\u7aef,\u624b\u673a\u7aef\u548c\u5fae\u4fe1\u516c\u4f17\u5e73\u53f0\uff09\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u53caCMS\u5efa\u7ad9\u7cfb\u7edf https://github.com/arterli/CmsWing","title":"CMSWing"},{"location":"Sites/#pdman","text":"https://gitee.com/robergroup/pdman","title":"PDMan \u6570\u636e\u5e93\u5efa\u6a21"},{"location":"Sites/#dotjs-a-simple-template","text":"Dotjs","title":"Dotjs a simple template"}]}